#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—  â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘  â•‘
â•‘   â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•šâ•â•  â•šâ•â•â•â•  â•‘
â•‘                                                                                      â•‘
â•‘                    MEGA ENGINE - HYPERVELOCITY CODEBASE INTELLIGENCE                 â•‘
â•‘                    M2 ULTRA 192GB â€¢ 24 CORES â€¢ ZERO LATENCY                          â•‘
â•‘                    GORUNFREE xâˆ â€¢ NOIZYLAB SUPREME                                   â•‘
â•‘                                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import gc
import sys
import json
import asyncio
import hashlib
import subprocess
import threading
import time
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import lru_cache
import multiprocessing

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYPERVELOCITY OPTIMIZATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    import uvloop
    uvloop.install()
    UVLOOP_ACTIVE = True
except ImportError:
    UVLOOP_ACTIVE = False

try:
    import orjson
    def json_dumps(obj): return orjson.dumps(obj, option=orjson.OPT_INDENT_2).decode()
    def json_loads(s): return orjson.loads(s)
    ORJSON_ACTIVE = True
except ImportError:
    json_dumps = lambda obj: json.dumps(obj, indent=2, default=str)
    json_loads = json.loads
    ORJSON_ACTIVE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM CONFIGURATION - M2 ULTRA OPTIMIZED
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CORES = os.cpu_count() or 24
RAM_GB = 192  # M2 Ultra
MAX_WORKERS = CORES * 4  # 96 workers
PROCESS_WORKERS = CORES  # 24 process workers
CACHE_SIZE_MB = 1024  # 1GB cache
SCAN_DEPTH_DEFAULT = 15
GREP_LIMIT_DEFAULT = 200
FILE_LIMIT_DEFAULT = 10000

NOIZYLAB_ROOT = Path("/Users/m2ultra/NOIZYLAB")
FISH_MUSIC_ROOT = Path("/Volumes/4TB Blue Fish")
CODEMASTER_ROOT = Path("/Users/m2ultra/NOIZYLAB/CODEMASTER")
CONFIG_DIR = Path.home() / ".noizylab"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LANGUAGE DETECTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LANGUAGE_MAP = {
    # Programming Languages
    '.py': 'Python', '.pyi': 'Python', '.pyw': 'Python',
    '.js': 'JavaScript', '.mjs': 'JavaScript', '.cjs': 'JavaScript',
    '.ts': 'TypeScript', '.tsx': 'TypeScript', '.jsx': 'JavaScript',
    '.go': 'Go', '.rs': 'Rust', '.c': 'C', '.cpp': 'C++', '.cc': 'C++',
    '.h': 'C Header', '.hpp': 'C++ Header',
    '.java': 'Java', '.kt': 'Kotlin', '.scala': 'Scala',
    '.swift': 'Swift', '.m': 'Objective-C', '.mm': 'Objective-C++',
    '.rb': 'Ruby', '.php': 'PHP', '.pl': 'Perl', '.pm': 'Perl',
    '.lua': 'Lua', '.r': 'R', '.jl': 'Julia',
    '.hs': 'Haskell', '.ml': 'OCaml', '.fs': 'F#', '.fsx': 'F#',
    '.ex': 'Elixir', '.exs': 'Elixir', '.erl': 'Erlang',
    '.clj': 'Clojure', '.cljs': 'ClojureScript',
    '.zig': 'Zig', '.nim': 'Nim', '.cr': 'Crystal',
    '.v': 'V', '.odin': 'Odin',
    
    # Web
    '.html': 'HTML', '.htm': 'HTML', '.xhtml': 'XHTML',
    '.css': 'CSS', '.scss': 'SCSS', '.sass': 'Sass', '.less': 'Less',
    '.vue': 'Vue', '.svelte': 'Svelte', '.astro': 'Astro',
    
    # Data & Config
    '.json': 'JSON', '.json5': 'JSON5',
    '.yaml': 'YAML', '.yml': 'YAML',
    '.toml': 'TOML', '.ini': 'INI', '.cfg': 'Config',
    '.xml': 'XML', '.xsl': 'XSLT', '.xsd': 'XSD',
    '.env': 'Environment',
    
    # Scripts
    '.sh': 'Shell', '.bash': 'Bash', '.zsh': 'Zsh',
    '.fish': 'Fish', '.ps1': 'PowerShell', '.bat': 'Batch', '.cmd': 'Batch',
    
    # Documentation
    '.md': 'Markdown', '.mdx': 'MDX', '.rst': 'reStructuredText',
    '.txt': 'Text', '.adoc': 'AsciiDoc',
    
    # Database
    '.sql': 'SQL', '.graphql': 'GraphQL', '.gql': 'GraphQL',
    '.prisma': 'Prisma',
    
    # DevOps & Build
    '.dockerfile': 'Docker', '.tf': 'Terraform', '.hcl': 'HCL',
    '.cmake': 'CMake', '.gradle': 'Gradle', '.groovy': 'Groovy',
    
    # Other
    '.proto': 'Protocol Buffers', '.thrift': 'Thrift',
    '.wasm': 'WebAssembly', '.wat': 'WebAssembly Text',
}

# Skip directories
SKIP_DIRS = {
    '.git', '.svn', '.hg', '.bzr',
    'node_modules', 'bower_components', 'jspm_packages',
    'venv', '.venv', 'env', '.env', 'virtualenv',
    '__pycache__', '.pytest_cache', '.mypy_cache', '.ruff_cache',
    '.tox', '.nox', '.cache', '.eggs', '*.egg-info',
    'dist', 'build', 'target', 'out', 'bin', 'obj',
    '.idea', '.vscode', '.vs', '.eclipse',
    'vendor', 'third_party', 'external',
    '.terraform', '.serverless',
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MEGA CACHE SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MegaCache:
    """High-performance in-memory cache with TTL"""
    
    def __init__(self, max_size_mb: int = CACHE_SIZE_MB):
        self.max_size = max_size_mb * 1024 * 1024
        self.cache: Dict[str, Tuple[Any, float, int]] = {}  # key: (value, expires, size)
        self.current_size = 0
        self.hits = 0
        self.misses = 0
        self.lock = threading.RLock()
    
    def _hash_key(self, key: str) -> str:
        return hashlib.md5(key.encode()).hexdigest()[:16]
    
    def get(self, key: str) -> Optional[Any]:
        hkey = self._hash_key(key)
        with self.lock:
            if hkey in self.cache:
                value, expires, size = self.cache[hkey]
                if expires > time.time():
                    self.hits += 1
                    return value
                else:
                    # Expired
                    del self.cache[hkey]
                    self.current_size -= size
            self.misses += 1
            return None
    
    def set(self, key: str, value: Any, ttl: int = 300):
        hkey = self._hash_key(key)
        serialized = json_dumps(value) if not isinstance(value, (str, bytes)) else str(value)
        size = len(serialized.encode()) if isinstance(serialized, str) else len(serialized)
        
        with self.lock:
            # Evict if needed
            while self.current_size + size > self.max_size and self.cache:
                oldest_key = next(iter(self.cache))
                _, _, old_size = self.cache.pop(oldest_key)
                self.current_size -= old_size
            
            self.cache[hkey] = (value, time.time() + ttl, size)
            self.current_size += size
    
    def clear(self):
        with self.lock:
            self.cache.clear()
            self.current_size = 0
    
    def stats(self) -> Dict:
        with self.lock:
            total = self.hits + self.misses
            return {
                "entries": len(self.cache),
                "size_mb": round(self.current_size / (1024 * 1024), 2),
                "max_size_mb": CACHE_SIZE_MB,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": f"{(self.hits / total * 100):.1f}%" if total > 0 else "0%"
            }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FILE INTELLIGENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class FileIntelligence:
    """Intelligence data for a single file"""
    path: str
    size: int
    lines: int
    language: str
    complexity: int = 0
    imports: List[str] = field(default_factory=list)
    functions: List[str] = field(default_factory=list)
    classes: List[str] = field(default_factory=list)
    todo_count: int = 0
    modified: str = ""
    
    def to_dict(self) -> Dict:
        return {
            "path": self.path,
            "size": self.size,
            "lines": self.lines,
            "language": self.language,
            "complexity": self.complexity,
            "imports": self.imports[:20],
            "functions": self.functions[:30],
            "classes": self.classes[:20],
            "todo_count": self.todo_count,
            "modified": self.modified,
        }


def analyze_file(file_path: Path) -> Optional[FileIntelligence]:
    """Deep analysis of a single file"""
    try:
        stat = file_path.stat()
        ext = file_path.suffix.lower()
        lang = LANGUAGE_MAP.get(ext, "Unknown")
        
        if stat.st_size > 5 * 1024 * 1024:  # Skip files > 5MB
            return FileIntelligence(
                path=str(file_path),
                size=stat.st_size,
                lines=0,
                language=lang,
                modified=datetime.fromtimestamp(stat.st_mtime).isoformat()
            )
        
        content = file_path.read_text(errors='ignore')
        lines = content.split('\n')
        
        intel = FileIntelligence(
            path=str(file_path),
            size=stat.st_size,
            lines=len(lines),
            language=lang,
            modified=datetime.fromtimestamp(stat.st_mtime).isoformat()
        )
        
        # Language-specific analysis
        if lang == 'Python':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('import ') or stripped.startswith('from '):
                    intel.imports.append(stripped[:80])
                elif stripped.startswith('def '):
                    name = stripped[4:].split('(')[0].strip()
                    intel.functions.append(name)
                    intel.complexity += 1
                elif stripped.startswith('class '):
                    name = stripped[6:].split('(')[0].split(':')[0].strip()
                    intel.classes.append(name)
                    intel.complexity += 2
                elif any(kw in stripped for kw in ['if ', 'for ', 'while ', 'try:', 'except']):
                    intel.complexity += 1
                if 'TODO' in line or 'FIXME' in line or 'HACK' in line:
                    intel.todo_count += 1
        
        elif lang in ('JavaScript', 'TypeScript'):
            for line in lines:
                stripped = line.strip()
                if 'import ' in stripped or 'require(' in stripped:
                    intel.imports.append(stripped[:80])
                elif 'function ' in stripped or '=>' in stripped:
                    intel.complexity += 1
                elif 'class ' in stripped:
                    intel.complexity += 2
                if 'TODO' in line or 'FIXME' in line:
                    intel.todo_count += 1
        
        elif lang == 'Go':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('import'):
                    intel.imports.append(stripped[:80])
                elif stripped.startswith('func '):
                    name = stripped[5:].split('(')[0].strip()
                    intel.functions.append(name)
                    intel.complexity += 1
                elif stripped.startswith('type ') and 'struct' in stripped:
                    intel.complexity += 2
        
        elif lang == 'Rust':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('use '):
                    intel.imports.append(stripped[:80])
                elif stripped.startswith('fn '):
                    name = stripped[3:].split('(')[0].strip()
                    intel.functions.append(name)
                    intel.complexity += 1
                elif stripped.startswith('struct ') or stripped.startswith('enum '):
                    intel.complexity += 2
        
        return intel
        
    except Exception:
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MEGA ENGINE CORE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MegaEngine:
    """
    MEGA ENGINE - Hypervelocity Codebase Intelligence
    Combines: TURBO MCP + HYPER MINER + GOD BRAIN + UNIFIED MCP
    """
    
    def __init__(self):
        self.cache = MegaCache()
        self.executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
        self.process_pool = ProcessPoolExecutor(max_workers=PROCESS_WORKERS)
        self.scan_results: Dict[str, Any] = {}
        self.active_scans = 0
        self.total_files_processed = 0
        self.start_time = time.time()
        
        # GC Optimization
        gc.disable()
        self._gc_frozen = True
    
    def status(self) -> Dict:
        """Get engine status"""
        return {
            "engine": "MEGA ENGINE",
            "version": "1.0.0",
            "status": "ONLINE",
            "optimizations": {
                "uvloop": UVLOOP_ACTIVE,
                "orjson": ORJSON_ACTIVE,
                "gc_frozen": self._gc_frozen,
            },
            "resources": {
                "cores": CORES,
                "ram_gb": RAM_GB,
                "thread_workers": MAX_WORKERS,
                "process_workers": PROCESS_WORKERS,
            },
            "cache": self.cache.stats(),
            "stats": {
                "files_processed": self.total_files_processed,
                "active_scans": self.active_scans,
                "uptime_seconds": round(time.time() - self.start_time, 2),
            }
        }
    
    async def turbo_scan(
        self, 
        path: str = str(NOIZYLAB_ROOT),
        depth: int = SCAN_DEPTH_DEFAULT,
        use_cache: bool = True
    ) -> Dict:
        """
        âš¡ TURBO SCAN - Parallel codebase scanning
        Returns comprehensive stats and intelligence
        """
        cache_key = f"scan:{path}:{depth}"
        if use_cache:
            cached = self.cache.get(cache_key)
            if cached:
                return {"source": "cache", **cached}
        
        self.active_scans += 1
        start_time = time.time()
        
        target = Path(path)
        if not target.exists():
            self.active_scans -= 1
            return {"error": f"Path not found: {path}"}
        
        stats = {
            "path": str(target),
            "timestamp": datetime.now().isoformat(),
            "total_files": 0,
            "total_lines": 0,
            "total_bytes": 0,
            "languages": defaultdict(lambda: {"files": 0, "lines": 0, "bytes": 0}),
            "complexity_score": 0,
            "hot_files": [],
            "large_files": [],
            "recent_files": [],
        }
        
        # Collect all files using native find (faster than os.walk)
        files = []
        try:
            cmd = f"find '{path}' -type f -maxdepth {depth} 2>/dev/null | head -n {FILE_LIMIT_DEFAULT}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)
            files = [f for f in result.stdout.strip().split('\n') if f and not any(skip in f for skip in SKIP_DIRS)]
        except Exception:
            # Fallback to os.walk
            for root, dirs, filenames in os.walk(target):
                dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
                curr_depth = str(root).count(os.sep) - str(target).count(os.sep)
                if curr_depth > depth:
                    continue
                for f in filenames:
                    if not f.startswith('.'):
                        files.append(str(Path(root) / f))
                        if len(files) >= FILE_LIMIT_DEFAULT:
                            break
        
        # Parallel file analysis
        all_intel: List[FileIntelligence] = []
        
        def process_batch(batch):
            results = []
            for fp in batch:
                intel = analyze_file(Path(fp))
                if intel:
                    results.append(intel)
            return results
        
        # Split into batches for parallel processing
        batch_size = max(100, len(files) // MAX_WORKERS)
        batches = [files[i:i + batch_size] for i in range(0, len(files), batch_size)]
        
        loop = asyncio.get_event_loop()
        futures = [loop.run_in_executor(self.executor, process_batch, batch) for batch in batches]
        
        for future in asyncio.as_completed(futures):
            batch_results = await future
            all_intel.extend(batch_results)
        
        # Aggregate stats
        for intel in all_intel:
            stats["total_files"] += 1
            stats["total_lines"] += intel.lines
            stats["total_bytes"] += intel.size
            stats["complexity_score"] += intel.complexity
            
            lang = intel.language
            stats["languages"][lang]["files"] += 1
            stats["languages"][lang]["lines"] += intel.lines
            stats["languages"][lang]["bytes"] += intel.size
        
        # Sort for hot files (by complexity)
        all_intel.sort(key=lambda x: x.complexity, reverse=True)
        stats["hot_files"] = [i.to_dict() for i in all_intel[:20]]
        
        # Large files
        large = sorted(all_intel, key=lambda x: x.size, reverse=True)[:10]
        stats["large_files"] = [{"path": i.path, "size_kb": round(i.size / 1024, 1), "lines": i.lines} for i in large]
        
        # Recent files (by modified time)
        recent = sorted(all_intel, key=lambda x: x.modified, reverse=True)[:10]
        stats["recent_files"] = [{"path": i.path, "modified": i.modified, "language": i.language} for i in recent]
        
        # Convert defaultdict to dict
        stats["languages"] = dict(stats["languages"])
        
        # Performance stats
        duration = time.time() - start_time
        stats["performance"] = {
            "duration_seconds": round(duration, 2),
            "files_per_second": round(stats["total_files"] / duration, 0) if duration > 0 else 0,
            "workers_used": MAX_WORKERS,
        }
        
        self.total_files_processed += stats["total_files"]
        self.active_scans -= 1
        
        # Cache result
        self.cache.set(cache_key, stats, ttl=600)  # 10 min TTL
        self.scan_results[path] = stats
        
        return {"source": "live", **stats}
    
    async def hyper_grep(
        self,
        pattern: str,
        path: str = str(NOIZYLAB_ROOT),
        limit: int = GREP_LIMIT_DEFAULT,
        case_sensitive: bool = False
    ) -> Dict:
        """
        âš¡ HYPER GREP - Parallel content search
        """
        cache_key = f"grep:{pattern}:{path}:{limit}:{case_sensitive}"
        cached = self.cache.get(cache_key)
        if cached:
            return {"source": "cache", **cached}
        
        start_time = time.time()
        target = Path(path)
        results = []
        
        p = pattern if case_sensitive else pattern.lower()
        code_ext = set(LANGUAGE_MAP.keys())
        
        # Collect searchable files
        files = []
        for root, dirs, filenames in os.walk(target):
            dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
            for f in filenames:
                fp = Path(root) / f
                if fp.suffix.lower() in code_ext:
                    files.append(fp)
                    if len(files) >= 5000:  # Limit for grep
                        break
        
        def search_file(fp: Path) -> List[Dict]:
            matches = []
            try:
                for i, line in enumerate(fp.read_text(errors='ignore').split('\n'), 1):
                    check_line = line if case_sensitive else line.lower()
                    if p in check_line:
                        matches.append({
                            "file": str(fp),
                            "line": i,
                            "text": line.strip()[:150],
                            "language": LANGUAGE_MAP.get(fp.suffix.lower(), "Unknown")
                        })
                        if len(matches) >= 10:  # Max per file
                            break
            except Exception:
                pass
            return matches
        
        # Parallel search
        loop = asyncio.get_event_loop()
        futures = [loop.run_in_executor(self.executor, search_file, fp) for fp in files]
        
        for future in asyncio.as_completed(futures):
            file_matches = await future
            results.extend(file_matches)
            if len(results) >= limit:
                break
        
        results = results[:limit]
        duration = time.time() - start_time
        
        response = {
            "pattern": pattern,
            "path": str(path),
            "matches": results,
            "total_matches": len(results),
            "files_searched": len(files),
            "duration_seconds": round(duration, 2),
        }
        
        self.cache.set(cache_key, response, ttl=300)
        return {"source": "live", **response}
    
    async def parallel_execute(
        self,
        commands: List[str],
        cwd: str = None,
        timeout: int = 30
    ) -> Dict:
        """
        âš¡ PARALLEL EXECUTE - Run multiple commands simultaneously
        """
        start_time = time.time()
        
        def run_cmd(cmd: str) -> Dict:
            try:
                r = subprocess.run(
                    cmd, shell=True, cwd=cwd,
                    capture_output=True, text=True, timeout=timeout
                )
                return {
                    "command": cmd,
                    "stdout": r.stdout[:5000],
                    "stderr": r.stderr[:1000],
                    "code": r.returncode,
                    "success": r.returncode == 0
                }
            except subprocess.TimeoutExpired:
                return {"command": cmd, "error": "timeout", "success": False}
            except Exception as e:
                return {"command": cmd, "error": str(e), "success": False}
        
        loop = asyncio.get_event_loop()
        futures = [loop.run_in_executor(self.executor, run_cmd, cmd) for cmd in commands]
        results = await asyncio.gather(*futures)
        
        duration = time.time() - start_time
        successes = sum(1 for r in results if r.get("success"))
        
        return {
            "results": results,
            "total": len(commands),
            "successes": successes,
            "failures": len(commands) - successes,
            "duration_seconds": round(duration, 2),
        }
    
    async def mine_codebase(
        self,
        source: str,
        destination: str,
        extensions: List[str] = None
    ) -> Dict:
        """
        âš¡ MINE CODEBASE - Extract and deduplicate code files
        """
        start_time = time.time()
        
        if extensions is None:
            extensions = list(LANGUAGE_MAP.keys())
        
        src = Path(source)
        dest = Path(destination)
        dest.mkdir(parents=True, exist_ok=True)
        
        copied = 0
        skipped = 0
        total_bytes = 0
        
        def process_file(src_path: Path) -> Tuple[int, int, int]:
            try:
                if not src_path.exists() or src_path.stat().st_size == 0:
                    return (0, 1, 0)
                
                try:
                    rel_path = src_path.relative_to(src)
                except ValueError:
                    rel_path = Path(src_path.name)
                
                dest_path = dest / rel_path
                
                if dest_path.exists():
                    if dest_path.stat().st_size == src_path.stat().st_size:
                        return (0, 1, 0)
                
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                
                import shutil
                shutil.copy2(src_path, dest_path)
                return (1, 0, src_path.stat().st_size)
                
            except Exception:
                return (0, 0, 0)
        
        # Collect files
        files = []
        for root, dirs, filenames in os.walk(src):
            dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
            for f in filenames:
                fp = Path(root) / f
                if fp.suffix.lower() in extensions:
                    files.append(fp)
        
        # Parallel processing
        loop = asyncio.get_event_loop()
        futures = [loop.run_in_executor(self.executor, process_file, fp) for fp in files]
        
        for future in asyncio.as_completed(futures):
            c, s, b = await future
            copied += c
            skipped += s
            total_bytes += b
        
        duration = time.time() - start_time
        
        return {
            "source": str(src),
            "destination": str(dest),
            "files_found": len(files),
            "files_copied": copied,
            "files_skipped": skipped,
            "bytes_transferred": total_bytes,
            "megabytes_transferred": round(total_bytes / (1024 * 1024), 2),
            "duration_seconds": round(duration, 2),
            "speed_files_per_sec": round(len(files) / duration, 0) if duration > 0 else 0,
        }
    
    async def warp_drive(self, target_url: str = "http://localhost:8000") -> Dict:
        """
        ğŸš€ WARP DRIVE - Cloudflare Tunnel Management
        """
        # Check cloudflared
        try:
            result = subprocess.run(
                ["cloudflared", "--version"],
                capture_output=True, text=True
            )
            cf_version = result.stdout.strip() if result.returncode == 0 else "Not installed"
        except FileNotFoundError:
            return {
                "status": "error",
                "message": "cloudflared not installed",
                "install": "brew install cloudflared"
            }
        
        return {
            "status": "ready",
            "cloudflared_version": cf_version,
            "target_url": target_url,
            "commands": {
                "quick_tunnel": f"cloudflared tunnel --url {target_url}",
                "login": "cloudflared login",
                "create_tunnel": "cloudflared tunnel create noizylab",
            }
        }
    
    def gc_control(self, action: str = "status") -> Dict:
        """GC Control - Manage garbage collection"""
        if action == "freeze":
            gc.disable()
            self._gc_frozen = True
            return {"status": "frozen", "gc_enabled": False}
        elif action == "thaw":
            gc.enable()
            gc.collect()
            self._gc_frozen = False
            return {"status": "thawed", "gc_enabled": True, "collected": gc.get_count()}
        elif action == "collect":
            was_enabled = gc.isenabled()
            gc.enable()
            collected = gc.collect()
            if not was_enabled:
                gc.disable()
            return {"status": "collected", "objects_freed": collected}
        else:
            return {
                "status": "frozen" if self._gc_frozen else "active",
                "gc_enabled": gc.isenabled(),
                "counts": gc.get_count(),
                "thresholds": gc.get_threshold(),
            }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLOBAL INSTANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEGA_ENGINE = MegaEngine()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    """CLI for MEGA ENGINE"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                      â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—  â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘  â•‘
â•‘   â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•šâ•â•  â•šâ•â•â•â•  â•‘
â•‘                                                                                      â•‘
â•‘                         HYPERVELOCITY CODEBASE INTELLIGENCE                          â•‘
â•‘                               GORUNFREE xâˆ â€¢ NOIZYLAB                                â•‘
â•‘                                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    status = MEGA_ENGINE.status()
    print(f"\nâš¡ ENGINE STATUS:")
    print(f"   Cores: {status['resources']['cores']}")
    print(f"   RAM: {status['resources']['ram_gb']}GB")
    print(f"   Workers: {status['resources']['thread_workers']} threads + {status['resources']['process_workers']} processes")
    print(f"   uvloop: {'âœ…' if status['optimizations']['uvloop'] else 'âŒ'}")
    print(f"   orjson: {'âœ…' if status['optimizations']['orjson'] else 'âŒ'}")
    print(f"   GC: {'â„ï¸ FROZEN' if status['optimizations']['gc_frozen'] else 'ğŸ”¥ ACTIVE'}")
    
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        
        if cmd == "scan":
            path = sys.argv[2] if len(sys.argv) > 2 else str(NOIZYLAB_ROOT)
            print(f"\nâš¡ TURBO SCANNING: {path}")
            result = await MEGA_ENGINE.turbo_scan(path)
            print(json_dumps(result))
        
        elif cmd == "grep":
            pattern = sys.argv[2] if len(sys.argv) > 2 else "TODO"
            path = sys.argv[3] if len(sys.argv) > 3 else str(NOIZYLAB_ROOT)
            print(f"\nâš¡ HYPER GREP: '{pattern}' in {path}")
            result = await MEGA_ENGINE.hyper_grep(pattern, path)
            print(json_dumps(result))
        
        elif cmd == "mine":
            source = sys.argv[2] if len(sys.argv) > 2 else str(FISH_MUSIC_ROOT)
            dest = sys.argv[3] if len(sys.argv) > 3 else "/tmp/mined_code"
            print(f"\nâš¡ MINING: {source} â†’ {dest}")
            result = await MEGA_ENGINE.mine_codebase(source, dest)
            print(json_dumps(result))
        
        elif cmd == "status":
            print(json_dumps(status))
        
        else:
            print(f"\nâŒ Unknown command: {cmd}")
            print("Commands: scan [path], grep <pattern> [path], mine <source> <dest>, status")
    else:
        print("\nğŸš€ MEGA ENGINE READY")
        print("Commands: scan [path], grep <pattern> [path], mine <source> <dest>, status")


if __name__ == "__main__":
    asyncio.run(main())
